## Bigdata Eco System

빅데이터는 수집, 정제, 적재, 분석, 시각화의 여러 단계를 거침

이 단계를 거치는 동안 여러가지 기술을 이용하여 처리되고, 이 기술들을 통틀어 빅데이터 에코 시스템이라고 함

​    

1. 수집 기술 - 빅데이터 분석을 위한 원천 데이터를 수집하는 기술
   - 플룸(Flume)
   - 카프카(Kafka)
   - NiFi
   - Sqoop
   - scribe
   - Fluentd
2. 작업 관리 기술 - 작업 관리 기술은 빅데이터를 분석하는 여러가지 단계를 효율적으로 생성, 관리하고 모니터할 수 있게 도와주는 기술
   - Airflow
   - Azkaban
   - Oozie
3. 데이터 직렬화 - 빅데이터 에코 시스템이 다양한 기술과 언어로 구현되기 때문에 각 언어간에 내부 객체를 공유해야 하는 경우, 이를 효율적으로 처리하기 위한 기술
   - Avro
   - Thrift
   - Protocol Buffers
4. 저장 - 빅데이터는 대용량의 데이터를 저장하기 때문에 데이터의 저장의 안정성과 속도가 중요
   - HDFS
   - S3
5. NoSQL
   - HBase
6. 데이터 처리 - 빅데이터를 분석하는 기술
   - MapReduce
   - Spark
   - Impala
   - Presto
   - Hive
   - Hcatalog
   - Pig
7. 클러스터 관리 - 빅데이터는 단일 시스템이 보다는 보통 클러스터로 처리 되기 때문에 자원의 효율적인 사용이 필요하며 이를 위한 여러가지 기술이 존재
   - YARN
   - Mesos
8. 분산 서버 관리 - 클러스터에서 여러가지 기술이 이용될 때 하나의 서버에서 모든 작업이 진행되면 이 서버가 단일실패지점(SPOF)이 됨. 이로 인한 리스크를 줄이기 위해 분산 서버 관리 기술을 이용
   - Zookeeper
9. 시각화
   - Zeppelin
   - Hue
10. 보안
    - Ranger
11. 데이터 거버넌스 - 기업의 여기저기 산재한 데이터를 같은 저장소에 관리, 비정형 데이터를 규칙에 맞게 표준화하는 전사 차원의 빅데이터 관리 체계
    - Atlas
    - Amundsen

​    

참고 : https://wikidocs.net/22651

​    

### Hadoop

- 대용량 데이터를 분산 처리할 수 있는 자바 기반의 오픈소스 프레임워크
- 분산저장 기술인 HDFS와 분산처리 기술인 맵리듀스(MapReduce)가 굉장한 장점

​    

#### Hadoop의 분산처리 단계

1. 클러스터에서 데이터 읽기
2. 동작 실행
3. 클러스터에 결과 기록
4. 데이터 업데이트 된 내용 읽기
5. 다음 동작 실행
6. 클러스터에 결과 기록

​    

### Apache spark

- 빅데이터 워크로드에 주로 사용되는 분산처리 시스템
- 빠른 성능을 위해 인 메모리 캐싱과 최적화 된 실행을 사용
- 일반 배치처리, 스트리밍 분석, 머신러닝, 그래프 데이터 베이스 및 임시 쿼리를 지원

​    

#### Apache spark의 분산처리 단계

1. 클러스터에서 데이터 읽기
2. 애널리틱스 운영 수행 및 결과값 클러스터 입력 동작과 같은 전 과정이 동시 진행

-> 일반적 상황에서 스파크의 데이터 처리속도는 하둡에 비해 월등히 빠름

​    

### Hadoop과 Apache spark

하둡은 HDFS(Hadoop Distributed File System)이라고 하는 분산형 파일 시스템과 프로세싱 컴포넌트인 맵리듀스를 제공

이 때문에 스파크가 필수적으로 필요하지 않음. 마찬가지로 스파크도 하둡의 HDFS 외에도 다른 클라우드 기반 데이터 플랫폼과도 융합 될 수 있어 하둡이 반드시 필요한 건 아님

다만 스파크 개발 당시에는 원래 하둡용으로 설계된 솔루션이기 때문에 하둡과 스파크를 함께 사용할 때 최상의 효과를 낼 수 있음

​    

